{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHLz4d9Et9ZM",
        "outputId": "0d8c1cef-f526-492e-ce03-39872316a9cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9XpFW24KVkc",
        "outputId": "ab6a3df3-8613-4187-be29-071d29127e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=fc89da9ec4b312f6bbe9f450742852a40c11e9808c3bf1ec9400ef071335a6c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FewE4zZPdgz",
        "outputId": "0db0c358-56b1-4d7b-adca-05ce07137f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9MuhFXFQaJy",
        "outputId": "035522a1-2c82-4344-e24b-755b13866801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLHfka8nA0Nq"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TORP4sD4qeM"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from textblob import TextBlob\n",
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "import re\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrW2HRV1AphQ"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of saving time and staying relatively within one era of hockey to avoid dramatic topic changes in posts, I chose to select the first 100000 observations from the dataset for training and testing."
      ],
      "metadata": {
        "id": "8-Px4J6Gssd4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jXfb94T3caY"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json('/content/drive/My Drive/filtered_posts.jsonl', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save this dataset to drive\n",
        "\n",
        "df.to_csv('/content/drive/My Drive/filtered_posts.csv', index=False)\n"
      ],
      "metadata": {
        "id": "8V_yN59Y7Q4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjz_mlpa-aY7"
      },
      "outputs": [],
      "source": [
        "# Function to check if a tweet is in English\n",
        "def is_english(text):\n",
        "    try:\n",
        "        return detect(text) == 'en'\n",
        "    except LangDetectException:\n",
        "        return False\n",
        "\n",
        "# Function to perform spell checking\n",
        "def spell_check(text):\n",
        "    blob = TextBlob(text)\n",
        "    return str(blob.correct())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UCrxE2X_nLp"
      },
      "outputs": [],
      "source": [
        "text_data = df.loc[:99999, ['title', 'created_utc']]\n",
        "text_data['title'] = df['title'].str.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-8i0bu8BOzb"
      },
      "source": [
        "## Spell check"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No matter the size of the dataset, spellcheck took more than 2 hours in most cases, and when I needed to restart my kernel, it made preprocessing an incredibly time-inefficient task."
      ],
      "metadata": {
        "id": "QVbAvIUOqh-B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siYGgtl8-q7q",
        "outputId": "c5ccf391-4aa3-4a43-81dd-8514222b363b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "let's play hockey - the hockey show\n",
            "let's play hockey - the hockey show\n"
          ]
        }
      ],
      "source": [
        "print(text_data['title'][20])\n",
        "print(spell_check(text_data['title'][20]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqXkrQTHA3Eg"
      },
      "outputs": [],
      "source": [
        "text_data['title'] = text_data['title'].apply(lambda x: spell_check(x) if is_english(x) else x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qixcmjpGDIAs"
      },
      "source": [
        "## Filtering out non-english"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same could be said for filtering out non-english posts. Reddit is typically more well-curated than twitter, meaning language is uniform and typically correct with few typos."
      ],
      "metadata": {
        "id": "E2_N31QqrL3r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa4h8gr4C_VQ"
      },
      "outputs": [],
      "source": [
        "text_data = text_data[text_data.apply(is_english)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzn4H1EpDWug"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to remove team names, as they seemed to muddy the waters when creating categories with LDA (each team name tended to be found in every category). I kept numbers as well because they provide meaning when talking about statistics in hockey."
      ],
      "metadata": {
        "id": "2dbYSxcUrYz6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bPLVCrFt_qQ"
      },
      "outputs": [],
      "source": [
        "teams = [\n",
        "    \"ducks\", \"coyotes\", \"bruins\", \"sabres\", \"flames\", \"hurricanes\",\n",
        "    \"blackhawks\", \"avalanche\", \"blue jackets\", \"stars\", \"red wings\",\n",
        "    \"oilers\", \"panthers\", \"kings\", \"wild\", \"canadiens\", \"predators\",\n",
        "    \"devils\", \"islanders\", \"rangers\", \"senators\", \"flyers\", \"penguins\",\n",
        "    \"sharks\", \"kraken\", \"blues\", \"lightning\", \"maple leafs\", \"canucks\",\n",
        "    \"golden knights\", \"capitals\", \"jets\", \"leafs\", \"knights\", \"jackets\", \"wings\"\n",
        "]\n",
        "\n",
        "teams_with_boundaries = [r'\\b' + team + r'\\b' for team in teams]\n",
        "\n",
        "teams_with_boundaries = '|'.join(teams_with_boundaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYEVu68UDQaj"
      },
      "outputs": [],
      "source": [
        "# Initialize lemmatizer and stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess tweet text for machine learning.\n",
        "\n",
        "    Done by removing URLs and special characters, tokenizing, removing\n",
        "    stop words, and lemmatizing the words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove \"&amp;\" an HTML entity\n",
        "    text = re.sub(r'&amp;', '', text)\n",
        "\n",
        "    # Remove hockey related words\n",
        "    text = re.sub(r'\\bhockey\\b|\\bnhl\\b|\\bv\\b|\\b'+teams_with_boundaries, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Initialize the lemmatizer and stop words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Remove stop words and lemmatize\n",
        "    processed_tokens = [\n",
        "        lemmatizer.lemmatize(token.lower())\n",
        "        for token in tokens if token.lower() not in stop_words\n",
        "    ]\n",
        "\n",
        "    return ' '.join(processed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP8oHsaADxOf"
      },
      "outputs": [],
      "source": [
        "text_data['preprocessed_title'] = text_data['title'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RtmSIpDFUOB"
      },
      "source": [
        "## Perform LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unl-9suRFW7D"
      },
      "outputs": [],
      "source": [
        "def perform_lda(series, num_topics=3, passes=15, no_below=2, no_above=0.5, top_words=10):\n",
        "    \"\"\"\n",
        "    Perform Latent Dirichlet Allocation (LDA) on a pandas Series of preprocessed text.\n",
        "\n",
        "    Parameters:\n",
        "    - series: pandas Series containing preprocessed text.\n",
        "    - num_topics: Number of topics to identify.\n",
        "    - passes: Number of passes through the corpus during training.\n",
        "    - no_below: Keep tokens which are contained in at least `no_below` documents.\n",
        "    - no_above: Keep tokens which are contained in no more than `no_above` documents (fraction of total corpus size).\n",
        "\n",
        "    Returns:\n",
        "    - topics: A list of topics with their top words.\n",
        "    \"\"\"\n",
        "\n",
        "    texts = [text.split() for text in series]\n",
        "\n",
        "    # Create a dictionary representation of the documents.\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "    # Filter out words that occur less than `no_below` documents, or more than `no_above` fraction of the documents.\n",
        "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
        "\n",
        "    # Create the corpus: a list of bags of words\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    # Build the LDA model\n",
        "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
        "\n",
        "    # Extract topics and their top words\n",
        "    topics = lda_model.print_topics(num_words=10)\n",
        "\n",
        "    return topics, lda_model, dictionary, corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrjmBlyyFmlv"
      },
      "outputs": [],
      "source": [
        "# apply LDA to the processed tweets\n",
        "topics, model, dictionary, corpus = perform_lda(text_data['preprocessed_title'], num_topics=4, passes=25, no_below=100, no_above=0.75, top_words=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should be noted that the topics produced in this notebook may exactly match the topics in the NN training notebook. This is because I continued to attenpt to refine my categories by tuning the LDA model after saving the first satisfactory dataset I recieved. I did save the topic dictionaries from that model and they are printed in a markdown cell in the NN training notebook. The most difficult part of this whole project, for me, was fitting an LDA model that provided good categories. This is partially because the data itself is hard to divide well into categories, as there are so many topics to discuss in the world of hockey that overlap in very nuanced ways, as well as changing drastically over time as new news arrives around the nhl and the sport as a whole."
      ],
      "metadata": {
        "id": "rg0DBwewtHzW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqJ86Oz0taRF"
      },
      "source": [
        "## Label Topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc9YnO4XFnKv"
      },
      "outputs": [],
      "source": [
        "# identify the topic for a tweet\n",
        "def get_topic_for_tweet(tweet, model, dictionary):\n",
        "    \"\"\"\n",
        "    Get the topic for a given tweet using the LDA model.\n",
        "\n",
        "    Parameters:\n",
        "    - tweet: The tweet text.\n",
        "    - model: The trained LDA model.\n",
        "    - dictionary: The dictionary used in the LDA model.\n",
        "\n",
        "    Returns:\n",
        "    - topic: The topic number assigned to the tweet.\n",
        "    \"\"\"\n",
        "    # Tweets should be preprosessed in the same way as the training data!!\n",
        "\n",
        "    # Convert the tweet to bag-of-words format\n",
        "    bow = dictionary.doc2bow(tweet.split())\n",
        "\n",
        "    # Get the topic distribution for the tweet\n",
        "    topic_distribution = model.get_document_topics(bow)\n",
        "\n",
        "    # Get the most probable topic\n",
        "    topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
        "\n",
        "    return topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO_4Na-etHok"
      },
      "outputs": [],
      "source": [
        "# add column in dataframe for topic\n",
        "text_data['topic'] = text_data['preprocessed_title'].apply(lambda x: get_topic_for_tweet(x, model, dictionary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcZyAfctvEaD"
      },
      "source": [
        "## Topic Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7gSkpBavDhz"
      },
      "outputs": [],
      "source": [
        "topics_df = pd.DataFrame(topics, columns=['Topic ID', 'Words'])\n",
        "# Convert the words in each topic to a dictionary of word frequencies\n",
        "topics_df['Words'] = topics_df['Words'].apply(lambda x: dict([word.split('*') for word in x.split(' + ')]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpFPUDvJvJvZ",
        "outputId": "86373827-1323-4408-fa2b-4c6ee9d0c013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: <br>\n",
            "-  0.042: \"player\"\n",
            "-  0.041: \"new\"\n",
            "-  0.027: \"team\"\n",
            "-  0.026: \"jersey\"\n",
            "-  0.020: \"r\"\n",
            "-  0.017: \"anyone\"\n",
            "-  0.016: \"league\"\n",
            "-  0.014: \"xpost\"\n",
            "-  0.013: \"get\"\n",
            "\n",
            "Topic 1: <br>\n",
            "-  0.042: \"year\"\n",
            "-  0.023: \"sign\"\n",
            "-  0.019: \"goal\"\n",
            "-  0.017: \"draft\"\n",
            "-  0.015: \"2\"\n",
            "-  0.014: \"deal\"\n",
            "-  0.012: \"hit\"\n",
            "\n",
            "Topic 2: <br>\n",
            "-  0.043: \"team\"\n",
            "-  0.039: \"fan\"\n",
            "-  0.023: \"one\"\n",
            "-  0.020: \"best\"\n",
            "-  0.018: \"like\"\n",
            "-  0.016: \"guy\"\n",
            "-  0.012: \"look\"\n",
            "-  0.011: \"trade\"\n",
            "\n",
            "Topic 3: <br>\n",
            "-  0.091: \"game\"\n",
            "-  0.030: \"cup\"\n",
            "-  0.026: \"playoff\"\n",
            "-  0.023: \"thread\"\n",
            "-  0.022: \"2013\"\n",
            "-  0.019: \"stanley\"\n",
            "-  0.017: \"last\"\n",
            "-  0.016: \"season\"\n",
            "-  0.015: \"time\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# display words and frequencies in each topic\n",
        "for index, row in topics_df.iterrows():\n",
        "    print(f\"Topic {row['Topic ID']}: <br>\")\n",
        "    for word, freq in row['Words'].items():\n",
        "        print(f\"-  {word}: {freq}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to save the CSV file in Google Drive.\n",
        "file_path = '/content/drive/My Drive/text_data.csv'\n",
        "\n",
        "# Save the DataFrame to a CSV file.\n",
        "text_data.to_csv(file_path, index=False)  # index=False prevents writing row indices to the file.\n",
        "\n",
        "print(f\"DataFrame saved to {file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F36eQCrS9rc",
        "outputId": "68c62082-d95f-4a4e-932c-003625f0405d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame saved to /content/drive/My Drive/text_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving test set\n",
        "test_hockey_data = df.loc[100000: 110000, ['title', 'created_utc']]\n",
        "test_hockey_data['title'] = df['title'].str.lower()\n",
        "\n",
        "file_path = '/content/drive/My Drive/test_hockey_data.csv'\n",
        "\n",
        "# Save the DataFrame to a CSV file.\n",
        "test_hockey_data.to_csv(file_path, index=False)  # index=False prevents writing row indices to the file."
      ],
      "metadata": {
        "id": "tqWKjzs-xFzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process tweets into tokens\n",
        "test_hockey_data['preprocessed_title'] = test_hockey_data['title'].apply(preprocess_text)\n",
        "print(test_hockey_data['preprocessed_title'][10:20]) # peek at some tweets"
      ],
      "metadata": {
        "id": "Y1YW06uczABJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}